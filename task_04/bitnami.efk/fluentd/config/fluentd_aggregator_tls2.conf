<system>
  log_level "#{ENV['FLUENTD_LOG_LEVEL']}"
</system>

<source>
  @type forward
  port 24224                       
  bind 0.0.0.0

  <security>
    self_hostname "#{ENV['SELF_HOSTNAME']}"      # ip-hostname
    shared_key "#{ENV['SHARED_KEY']}"            # Shared key (same as on Fluent Bit)
  </security>

  <transport tls> 
    cert_path /opt/bitnami/fluentd/certs/server.crt  
    private_key_path /opt/bitnami/fluentd/certs/server.key  
    ca_path /opt/bitnami/fluentd/certs/ca-cert.pem 
  </transport>
</source>
 
<label @FLUENT_LOG> 
  <match /^(?!fluent.debug).*/ >
    @type elasticsearch
    log_es_400_reason true
    host "#{ENV['ELASTIC_HOST']}"
    port 9200
    index_name ${tag}
    scheme https
    ssl_verify true
    logstash_format "#{ENV['LOGSTASH_FORMAT']}"
    logstash_prefix ${tag}
    user "#{ENV['FLUENTD_USER']}"
    password "#{ENV['FLUENTD_PASSWORD']}"
    flush_interval 1s
  </match>
</label>

<match **>
  # @type copy
  # <store>
    @type elasticsearch
    log_es_400_reason true
    host "#{ENV['ELASTIC_HOST']}"
    port 9200
    index_name ${tag}
    scheme https
    ssl_verify true
    logstash_format "#{ENV['FLUENTD_LOGSTASH_FORMAT']}"
    logstash_prefix ${tag}
    user "#{ENV['FLUENTD_USER']}"
    password "#{ENV['FLUENTD_PASSWORD']}"
    flush_interval 1s
    <buffer>
      @type file
      path /opt/bitnami/fluentd/buffer
      flush_interval 1s
      retry_max_interval 60s
      chunk_limit_size "#{ENV['FLUENTD_CHUNK_LIMIT_SIZE']}"
      total_limit_size "#{ENV['FLUENTD_TOTAL_LIMIT_SIZE']}"
      queued_chunks_limit_size "#{ENV['FLUENTD_QUEUED_CHUNKS_LIMIT_SIZE']}"
      flush_thread_count "#{ENV['FLUENTD_FLUSH_THREAD_COUNT']}"
    </buffer>
  # </store>
</match>