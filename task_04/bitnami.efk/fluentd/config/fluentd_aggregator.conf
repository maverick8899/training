<system>
  log_level "#{ENV['FLUENTD_LOG_LEVEL']}"
</system>

<source>
  @type forward
  port 24224                       
  bind 0.0.0.0
</source>
 
<filter /^(?!fluent.*\.).*/ >
  @type concat
  key log
  multiline_start_regexp /\[\d{2}:\d{2}:\d{2} \w+\](.*)/
  # continuous_line_regexp /^[A-Z].*/
  # multiline_end_regexp /[\s\S]*/
  # separator "\n"
  flush_interval 5 #? concat logs
  timeout_label "@timeout" 
</filter>
 
 
<label @timeout >
  <filter **>
    @type parser
    key_name log
    <parse>
      @type regexp
      expression /\[(?<timestamp>\d{2}:\d{2}:\d{2}) (?<log_lv>\w+)\] (?<msg>[\s\S]*)/
    </parse>
  </filter> 

  <match **>
    @type elasticsearch
    host "#{ENV['ELASTIC_HOST']}"
    port 9200
    index_name ${tag}
    scheme https
    ssl_verify true
    logstash_format true
    logstash_prefix ${tag} 
    user "#{ENV['ELASTIC_USERNAME']}"
    password "#{ENV['ELASTIC_PASSWORD']}"
    flush_interval 1s
    <buffer>
      @type file
      path /opt/bitnami/fluentd/cache/timeout
      flush_interval 1s
      retry_max_interval 30s
      chunk_limit_size 8MB
      queued_chunks_limit_size 128
    </buffer>
  </match>
 </label>

<filter /^(?!fluent.*\.).*/ >
  @type parser
  key_name log
  <parse>
    @type regexp
    expression /\[(?<timestamp>\d{2}:\d{2}:\d{2}) (?<log_lv>\w+)\] (?<msg>[\s\S]*)/
  </parse>
</filter> 

# <match **>
#   type stdout
# </match>

<match ** >
  @type elasticsearch
  host "#{ENV['ELASTIC_HOST']}"
  port 9200
  index_name ${tag}      
  scheme https
  ssl_verify true
  logstash_format true
  logstash_prefix ${tag}  #? default: logstash-<date> -> fluentd-<date>
  # include_tag_key true #? include the tag above
  # tag_key @log_name #? rename tag into "@log_name"
  user  "#{ENV['ELASTIC_USERNAME']}"
  password  "#{ENV['ELASTIC_PASSWORD']}"
  flush_interval 1s
  <buffer>
    @type file
    path /opt/bitnami/fluentd/cache
    flush_interval 1s #? interval send buffer
    retry_max_interval 30s #? base on backoff mechanism with retry_wait default 1s
    chunk_limit_size 8MB
    queued_chunks_limit_size 128 #? 128 chunks×8 MB=1024 MB (1 GB) = 1 buffer
  </buffer>
</match>